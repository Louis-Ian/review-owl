{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data embedding, vector uploading and PR info mapping\n",
    "## Data pipeline\n",
    "The task of this notebook is to ingest each Pull Request(PR) in the subdatasets, upload the relevant content to a Database, embed it using a feature extraction model, upload it to a vector database, and map the PR content to the corresponding vector for Retrieval Augmented Generation (RAG).\n",
    "Given the size of the overall dataset (over 30GB), this notebook will focus on the JavaScript portion of the dataset as a proof of concept.\n",
    "\n",
    "`create a queue of the file names -> then spawn N threads -> each thread takes a file from the queue -> generates the proper datastructure/dataframe from the json file -> sends to the inference api (or runs the transformer locally) for embeddings -> takes the resulting embedding -> generates the proper datastructure to upload to a vector database (pinecone) and does so in chuncks of 100 vectors, as per the pinecone documentation -> upload the corresponding PR info to another Database mapping for the VectorID -> when finished with a file save to a log -> repeat.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API keys and other secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='embedding_progress.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n",
    "POSTGRESQL_USER = os.getenv('POSTGRESQL_USER')\n",
    "POSTGRESQL_PASSWORD = os.getenv('POSTGRESQL_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "\n",
    "conn = pg.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=POSTGRESQL_USER,\n",
    "    password=POSTGRESQL_PASSWORD,\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "# Create a cursor object for SQL operations.\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PostgreSQL 16.0, compiled by Visual C++ build 1935, 64-bit',)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test connection\n",
    "cur.execute(\"SELECT version();\")\n",
    "cur.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"ROLLBACK\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a database for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the database already exists, and create it if not.\n",
    "cur.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'review_owl';\")\n",
    "exists = cur.fetchall()\n",
    "\n",
    "if not exists:\n",
    "    logging.info(\"Creating database\")\n",
    "    cur.execute(\"CREATE DATABASE review_owl;\")\n",
    "    conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the project database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the new database.\n",
    "conn = pg.connect(\n",
    "    dbname=\"review_owl\",\n",
    "    user=POSTGRESQL_USER,\n",
    "    password=POSTGRESQL_PASSWORD,\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "# Create a cursor object for SQL operations.\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Table for PR Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the table already exists, and create it if not.\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS pr_info (\n",
    "        pr_vector_id TEXT PRIMARY KEY,\n",
    "        pr_id INT UNIQUE,\n",
    "        pr_repo_name TEXT,\n",
    "        pr_html_url TEXT,\n",
    "        pr_file_path TEXT,\n",
    "        pr_line INT,\n",
    "        pr_user TEXT,\n",
    "        pr_diff_hunk TEXT,\n",
    "        pr_body TEXT,\n",
    "        pr_commit_id TEXT,\n",
    "        pr_language TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for inserting and updating rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert or update rows in the pr_info table.\n",
    "def insert_or_update_pr_info(vector_id: str, pr_id: int, repo_name: str, html_url: str, file_path: str, line: int, user: str, diff_hunk: str, body: str, commit_id: str, language: str):\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO pr_info (pr_vector_id, pr_id, pr_repo_name, pr_html_url, pr_file_path, pr_line, pr_user, pr_diff_hunk, pr_body, pr_commit_id, pr_language)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (pr_vector_id)\n",
    "            DO UPDATE SET\n",
    "                pr_id = EXCLUDED.pr_id,\n",
    "                pr_repo_name = EXCLUDED.pr_repo_name,\n",
    "                pr_html_url = EXCLUDED.pr_html_url,\n",
    "                pr_file_path = EXCLUDED.pr_file_path,\n",
    "                pr_line = EXCLUDED.pr_line,\n",
    "                pr_user = EXCLUDED.pr_user,\n",
    "                pr_diff_hunk = EXCLUDED.pr_diff_hunk,\n",
    "                pr_body = EXCLUDED.pr_body,\n",
    "                pr_commit_id = EXCLUDED.pr_commit_id,\n",
    "                pr_language = EXCLUDED.pr_language;;\n",
    "        \"\"\", (vector_id, pr_id, repo_name, html_url, file_path, line, user, diff_hunk, body, commit_id, language))\n",
    "        conn.commit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        logging.error(e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test insert\n",
    "# insert_or_update_pr_info(\"\", 44735997, \"plotly@plotly.js\", \"https://github.com/plotly/plotly.js/pull/1#discussion_r44735997\", \"devtools/test_dashboard/server.js\", 36, \"etpinard\", \"@@ -1,89 +1,53 @@\\n-var http = require('http');\\n-var ecstatic = require('ecstatic');\\n-var browserify = require('browserify');\\n-var open = require('open');\\n var fs = require('fs');\\n-var watchify = require('watchify');\\n+var http = require('http');\\n var path = require('path');\\n-var outpipe = require('outpipe');\\n-var outfile = path.join(__dirname, '../shelly/plotlyjs/static/plotlyjs/build/plotlyjs-bundle.js');\\n-\\n-var testFile = './test';\\n-\\n-switch(process.argv[2]) {\\n-  case 'geo':\\n-    testFile = './test-geo';\\n-  break;\\n-  case '2d':\\n-    testFile = './test-2d';\\n-  break;\\n-}\\n-\\n-console.log('using ' + testFile);\\n-\\n-var b = browserify(path.join(__dirname, '../shelly/plotlyjs/static/plotlyjs/src/plotly.js'), {\\n-  debug: true,\\n-  verbose: true,\\n-  standalone: 'Plotly',\\n-  transform: path.join(__dirname, '../shelly/plotlyjs/static/plotlyjs/compress_attributes.js'),\\n-  cache: {},\\n-  packageCache: {}\\n-});\\n-\\n-\\n-var w = watchify(b);\\n \\n-var bytes, time;\\n-w.on('bytes', function (b) { bytes = b });\\n-w.on('time', function (t) { time = t });\\n-\\n-w.on('update', bundle);\\n-bundle();\\n+var browserify = require('browserify');\\n+var ecstatic = require('ecstatic');\\n+var _open = require('open');\\n \\n-var firstBundle = true;\\n+var makeWatchifiedBundle = require('../../tasks/util/make_watchified_bundle');\\n+var shortcutPaths = require('../../tasks/util/shortcut_paths');\\n+var constants = require('../../tasks/util/constants');\\n \\n-function bundle () {\\n-    var didError = false;\\n-    var outStream = process.platform === 'win32'\\n-        ? fs.createWriteStream(outfile)\\n-        : outpipe(outfile);\\n \\n-    var wb = w.bundle();\\n-    wb.on('error', function (err) {\\n-        console.error(String(err));\\n-        didError = true;\\n-        outStream.end('console.error('+JSON.stringify(String(err))+');');\\n-    });\\n-    wb.pipe(outStream);\\n+// TODO make this an optional argument\\n+var PORT = '8080';\\n \\n-    outStream.on('error', function (err) {\\n-        console.error(err);\\n-    });\\n-    outStream.on('close', function () {\\n-        if (!didError) {\\n-            console.error(bytes + ' bytes written to ' + outfile\\n-                + ' (' + (time / 1000).toFixed(2) + ' seconds)'\\n-            );\\n-            if(firstBundle) {\\n-              open('http://localhost:8080/test-dashboard');\\n-              firstBundle = false;\\n-            }\\n-        }\\n-    });\\n+var testFile;\\n+switch(process.argv[2]) {\\n+    case 'geo':\\n+        testFile = './test-geo';\\n+    break;\\n+    case '2d':\\n+        testFile = './test-2d';\\n+    break;\\n+    default:\\n+        testFile = './test-3d';\\n }\\n \\n-////// build the test examples\\n+console.log('Using ' + testFile);\\n+console.log('Listening on :' + PORT + '\\\\n');\\n \\n-fs.unlink('./test-bundle.js', function(error) {\\n-    browserify({\\n+// watch plotly.js\\n+var watchifiedBundle = makeWatchifiedBundle(function onFirstBundleCallback() {\\n+    _open('http://localhost:' + PORT + '/devtools/test_dashboard');\\n+});\\n+watchifiedBundle();\", \"the test dashboard server script uses the same watch-bundling machinery as `npm run watch` \\n:palm_tree: :palm_tree: \\n\", \"2abf31f5cb19b7cca9a4944ee506fe716ec44442\", \"JavaScript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_info_by_vector_ids(vector_ids: list[str]):\n",
    "    placeholders = ', '.join(['%s' for _ in vector_ids])\n",
    "\n",
    "    sql = f\"SELECT * FROM pr_info WHERE vector_id IN ({placeholders})\"\n",
    "\n",
    "    cur.execute(sql, vector_ids)\n",
    "\n",
    "    results = cur.fetchall()\n",
    "\n",
    "    pr_info_list = []\n",
    "    for result in results:\n",
    "        pr_vector_id, pr_id, pr_repo_name, pr_html_url, pr_file_path, pr_line, pr_user, pr_diff_hunk, pr_body, pr_commit_id, pr_language = result\n",
    "        pr_info_list.append({\n",
    "            \"vector_id\": pr_vector_id,\n",
    "            \"id\": pr_id,\n",
    "            \"repo_name\": pr_repo_name,\n",
    "            \"html_url\": pr_html_url,\n",
    "            \"file_path\": pr_file_path,\n",
    "            \"line\": pr_line,\n",
    "            \"user\": pr_user,\n",
    "            \"diff_hunk\": pr_diff_hunk,\n",
    "            \"body\": pr_body,\n",
    "            \"commit_id\": pr_commit_id,\n",
    "            \"language\": pr_language\n",
    "        })\n",
    "\n",
    "    return pr_info_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=str(PINECONE_API_KEY),\n",
    "    environment='gcp-starter'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case we need to delete and recreate the index to start fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone.delete_index('review-owl')\n",
    "# pinecone.create_index('review-owl', dimension=384, metric='euclidean', pods=1, pod_type='starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.03892,\n",
       " 'namespaces': {'': {'vector_count': 3892}},\n",
       " 'total_vector_count': 3892}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_POOL_THREADS = 30\n",
    "index = pinecone.Index('review-owl', pool_threads=PINECONE_POOL_THREADS)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the models for running feature extraction locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# # model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "# model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "# # alternative embedding model to consider\n",
    "# # model = SentenceTransformer('BAAI/llm-embedder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing a specific repo dataset by finding all the files starting with the index number and a dash\n",
    "import glob\n",
    "\n",
    "# getting the index number from the file name\n",
    "def get_index_number(file):\n",
    "    \"\"\"A helper function to get the index number from the file name.\"\"\"\n",
    "    return int(file.split('\\\\')[1].split('-')[0])\n",
    "\n",
    "def import_filepaths(folder_path: str):\n",
    "    # getting all the files in the directory\n",
    "    file_paths = glob.glob(folder_path)\n",
    "\n",
    "    # sorting the files by the index number\n",
    "    sorted_filepaths = sorted(file_paths, key=get_index_number)\n",
    "\n",
    "    return sorted_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-base-en-v1.5\"\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-small-en-v1.5\"\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/BAAI/llm-embedder\"\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}\n",
    "\n",
    "def file_to_embedding_inputs(file_path: str):\n",
    "    embedding_input = []\n",
    "    \n",
    "    data = pd.read_json(file_path, orient='index')\n",
    "    for row in tqdm(data.iloc[:, 0], 'Splitting file into payloads'):\n",
    "        embedding_input.append('path: ' + row['path'] + '\\n' + 'diff_hunk: ' + row['diff_hunk'])\n",
    "    \n",
    "    return embedding_input\n",
    "\n",
    "def huggingface_inference_api_request(payload: list[str], payload_size: int = 1000):\n",
    "    try:\n",
    "        rejoined_list = []\n",
    "        for i in tqdm(range(0, len(payload), payload_size), 'Making requests to HuggingFace API'):\n",
    "            payload_slice = payload[i:i + payload_size]\n",
    "            data = json.dumps(payload_slice)\n",
    "            response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "\n",
    "            if ('Rate limit reached' in response.content.decode(\"utf-8\")):\n",
    "                raise Exception('Rate limit reached')\n",
    "            while (response.status_code != 200 and 'is currently loading' in response.content.decode(\"utf-8\")):\n",
    "                logging.info(f'Waiting for 30 seconds. Reponse: {response.content.decode(\"utf-8\")}')\n",
    "                time.sleep(30)\n",
    "                response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "\n",
    "            for v in json.loads(response.content.decode(\"utf-8\")):\n",
    "                rejoined_list.append(v)\n",
    "\n",
    "        return rejoined_list\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "def make_huggingface_request_with_backoff(payload: list[str]):\n",
    "    max_retries = 10\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = huggingface_inference_api_request(payload, 200)\n",
    "            if (response is Exception): raise response\n",
    "            return response\n",
    "        except requests.HTTPError as e:\n",
    "            print('HTTP error: ' + str(e.response.status_code))\n",
    "            if e.response.status_code == 503:\n",
    "                wait_time = (2 ** retries) + (random.uniform(0, 1) * 0.1)  # Exponential backoff with random jitter\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "            else:\n",
    "                print('Error: ' + str(e))\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            if (e == 'Rate limit reached'):\n",
    "                print('Rate limit reached, waiting for 10 minutes')\n",
    "                time.sleep(60 * 10) # wait for 10 minutes to try again\n",
    "                retries = 0\n",
    "                pass\n",
    "            logging.error(e)\n",
    "            retries += 1\n",
    "\n",
    "    logging.error('Max retries exceeded')\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for generating the info that will be saved in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_id(id):\n",
    "    return hashlib.sha256(id.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Function to check if vector ID matches PR information.\n",
    "def check_vector_id_match(repo_name: str, pr_id: int, vector_id: str):\n",
    "    gen_id = repo_name + '-' + hash_id(repo_name + str(pr_id))\n",
    "    return gen_id == vector_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pr_info(file_path: str, generated_vector_data: list[tuple[str, list, dict]], language: str):\n",
    "    pr_info = []\n",
    "\n",
    "    data = pd.read_json(file_path, orient='index')\n",
    "\n",
    "    for i, row in tqdm(enumerate(data.iloc[:, 0]), 'Generating PR info'):\n",
    "        if (check_vector_id_match(data.columns[0], row['id'], generated_vector_data[i][0])):\n",
    "            # (vector_id, pr_id, repo_name, html_url, file_path, line, user, diff_hunk, body, commit_id, language)\n",
    "            pr_info.append({'vector_id': generated_vector_data[i][0], 'pr_id': row['id'], 'repo_name': data.columns[0], 'html_url': row['html_url'], 'file_path': row['path'], 'line': row['line'], 'user': row['user'], 'diff_hunk': row['diff_hunk'], 'body': row['body'], 'commit_id': row['commit_id'], 'language': language})\n",
    "        else:\n",
    "            raise Exception('Vector IDs dont match')\n",
    "\n",
    "    return pr_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that go from raw data to processed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "def measure_metadata_size(metadata_list: list[dict]):\n",
    "    return [len(str(metadata)) for metadata in metadata_list]\n",
    "\n",
    "def generate_metadata(file_path: str):\n",
    "    metadata_list = []\n",
    "    data = pd.read_json(file_path, orient='index')\n",
    "\n",
    "    for row in data.iloc[:, 0]:\n",
    "        metadata_list.append({'id': row['id'],'repo': data.columns[0],'path': row['path']})\n",
    "    \n",
    "    metadata_size_list = measure_metadata_size(metadata_list)\n",
    "    for metadata_size in metadata_size_list:\n",
    "        if (metadata_size > 40960):\n",
    "            logging.error(f\"Metadata size is greater than 40960 bytes at {metadata_size} bytes at index: {metadata_size_list.index(metadata_size)}.\\nfile: {file_path}.\")\n",
    "\n",
    "    return metadata_list\n",
    "\n",
    "def generate_vector_ids(metadata_list: list[dict]):\n",
    "    ids = []\n",
    "    for i in range(len(metadata_list)):\n",
    "        ids.append(metadata_list[i]['repo'] + '-' + hash_id(metadata_list[i]['repo'] + str(metadata_list[i]['id'])))\n",
    "    return ids\n",
    "\n",
    "def embed_file(file_path: str):\n",
    "    vectors_list = []\n",
    "    embedding_inputs_list = file_to_embedding_inputs(file_path)\n",
    "\n",
    "    vectors_list = make_huggingface_request_with_backoff(embedding_inputs_list)\n",
    "\n",
    "    # code for running the embedding model locally\n",
    "    # for payload in tqdm(embedding_inputs_list, 'Generating vector embeddings'):\n",
    "        # vectors_list.append(model.encode(payload).tolist())\n",
    "    \n",
    "    return vectors_list\n",
    "\n",
    "def generate_upsert_data(file_path: str):\n",
    "    embedding_result = embed_file(file_path)\n",
    "    metadata_list = generate_metadata(file_path)\n",
    "    vector_ids = generate_vector_ids(metadata_list)\n",
    "\n",
    "    upsert_data = list(zip(vector_ids, embedding_result, metadata_list))\n",
    "\n",
    "    return upsert_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together with multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(iterable: list, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunks = []\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        chunks.append(list(itertools.islice(it, batch_size)))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom queue class to track the number of files\n",
    "class CountedQueue(queue.Queue):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.total_files = 0\n",
    "\n",
    "    def put(self, item, *args, **kwargs):\n",
    "        super().put(item, *args, **kwargs)\n",
    "        self.total_files += 1\n",
    "            \n",
    "def worker(file_queue: queue.Queue):\n",
    "    while True:\n",
    "        file_path = file_queue.get()\n",
    "        if file_path is None:\n",
    "            break\n",
    "\n",
    "        upsert_data = generate_upsert_data(file_path)\n",
    "\n",
    "        upsert_data_chunks = split_into_chunks(upsert_data)\n",
    "\n",
    "        for chunk in tqdm(upsert_data_chunks, 'Uploading data to Pinecone'):\n",
    "            index.upsert(chunk)\n",
    "\n",
    "        for row in generate_pr_info(file_path, upsert_data, 'JavaScript'):\n",
    "            # (vector_id, pr_id, repo_name, html_url, file_path, line, user, diff_hunk, body, commit_id, language)\n",
    "            insert_or_update_pr_info(row['vector_id'], row['pr_id'], row['repo_name'], row['html_url'], row['file_path'], row['line'], row['user'], row['diff_hunk'], row['body'], row['commit_id'], row['language'])\n",
    "        \n",
    "        split_txt = \"\\\\\"\n",
    "        logging.info(f\"Logging: {file_path.split(split_txt)[1]} processed\")\n",
    "\n",
    "        file_queue.task_done()\n",
    "\n",
    "# Function to create and manage worker threads\n",
    "def create_worker_threads(num_threads: int, file_queue: queue.Queue):\n",
    "    threads = list[threading.Thread]()\n",
    "\n",
    "    for _ in range(num_threads):\n",
    "        thread = threading.Thread(target=worker, args=(file_queue,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    return threads\n",
    "\n",
    "# Main function to process files using multiple threads\n",
    "def process_files_with_threads(file_names: list[str], num_threads: int, start: int = 0, end=None):\n",
    "    # Create a thread-safe queue\n",
    "    file_queue = CountedQueue()\n",
    "\n",
    "    # Populate the queue with file names\n",
    "    for file_name in file_names[start:end]:\n",
    "        file_queue.put(file_name)\n",
    "\n",
    "    # Create worker threads\n",
    "    threads = create_worker_threads(num_threads, file_queue)\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    with tqdm(total=file_queue.total_files, unit=\"file\") as pbar:\n",
    "        while pbar.n < file_queue.total_files:\n",
    "            pbar.update(0)  # Update the progress bar\n",
    "\n",
    "    # Wait for all file processing to be completed\n",
    "    file_queue.join()\n",
    "\n",
    "    # Stop the worker threads\n",
    "    for _ in range(num_threads):\n",
    "        file_queue.put(None)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    logging.info(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For testing\n",
    "# file_paths = import_filepaths('D:/GitHub/review-owl-datasets/dataset/mined-comments-25stars-25prs-JavaScript.json/repo-split/*.json')\n",
    "# e0 = file_to_embedding_inputs(file_paths[500])\n",
    "# u1 = generate_upsert_data(file_paths[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e1 = huggingface_inference_api_request(e0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1 = generate_metadata(file_paths[500])\n",
    "# type(m1[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pri1 = generate_pr_info(file_paths[500], u1, 'JavaScript')\n",
    "# pri1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1[0]['repo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 = generate_vector_ids(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z1 = list(zip(v1, e1, m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(z1))\n",
    "# print(type(z1[0]))\n",
    "# print(type(z1[0][0]))\n",
    "# print(type(z1[0][1]))\n",
    "# print(type(z1[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_id(m1[0]['repo'] + '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsert_data_chunks = split_into_chunks(z1)\n",
    "\n",
    "# for chunk in tqdm(upsert_data_chunks, 'Uploading data to Pinecone'):\n",
    "#     index.upsert(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsert_data = generate_upsert_data(file_paths[0])\n",
    "# upsert_data_chunks = split_into_chunks(upsert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.03916,\n",
       " 'namespaces': {'': {'vector_count': 3916}},\n",
       " 'total_vector_count': 3916}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fc0da8ceda4f5a8111ff4f19de42a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7366 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d054cf8b65334b5f991670222d4e3904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting file into payloads:   0%|          | 0/8576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7163d9ef173f4c0d9c750272a70e3619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting file into payloads:   0%|          | 0/8545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f938fc9a0f404ea62f5e71879cb707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting file into payloads:   0%|          | 0/9224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a034052eaa341afa576cda5642cdeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting file into payloads:   0%|          | 0/8859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6289c0540840d5872d6c35571b0ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making requests to HuggingFace API:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f7d14ca3af4925a13d05c49f785831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting file into payloads:   0%|          | 0/9083 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0a5c196d6049a5af24020b91812f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting file into payloads:   0%|          | 0/8230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eac758a50f45b6923cde608795704d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making requests to HuggingFace API:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecfc2127f4e4f1f981cfa24b618da57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making requests to HuggingFace API:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3163045991d477281620fff5874687d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making requests to HuggingFace API:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c244e6ceb9d84f359bfb2eb0d484ba6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making requests to HuggingFace API:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76c1f727353426d85127aaf6fe5c840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making requests to HuggingFace API:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\GitHub\\review-owl\\github-dataset-embedding-and-uploading.ipynb Cell 50\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitHub/review-owl/github-dataset-embedding-and-uploading.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m file_paths \u001b[39m=\u001b[39m import_filepaths(\u001b[39m'\u001b[39m\u001b[39mD:/GitHub/review-owl-datasets/dataset/mined-comments-25stars-25prs-JavaScript.json/repo-split/*.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/GitHub/review-owl/github-dataset-embedding-and-uploading.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m process_files_with_threads(file_names\u001b[39m=\u001b[39mfile_paths, num_threads\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n",
      "\u001b[1;32mc:\\GitHub\\review-owl\\github-dataset-embedding-and-uploading.ipynb Cell 50\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/review-owl/github-dataset-embedding-and-uploading.ipynb#X32sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Create a tqdm progress bar\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/review-owl/github-dataset-embedding-and-uploading.ipynb#X32sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mfile_queue\u001b[39m.\u001b[39mtotal_files, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GitHub/review-owl/github-dataset-embedding-and-uploading.ipynb#X32sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m pbar\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m file_queue\u001b[39m.\u001b[39mtotal_files:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/review-owl/github-dataset-embedding-and-uploading.ipynb#X32sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39m0\u001b[39m)  \u001b[39m# Update the progress bar\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/review-owl/github-dataset-embedding-and-uploading.ipynb#X32sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Wait for all file processing to be completed\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_paths = import_filepaths('D:/GitHub/review-owl-datasets/dataset/mined-comments-25stars-25prs-JavaScript.json/repo-split/*.json')\n",
    "process_files_with_threads(file_names=file_paths, num_threads=6, start=5000, end=5010)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
