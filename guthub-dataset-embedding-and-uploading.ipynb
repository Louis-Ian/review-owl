{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data embedding and vector uploading\n",
    "`create a queue of the file names -> then spawn 10 threads -> each thread takes a file from the queue -> generates the proper datastructure/dataframe from the json file -> sends to the inference api or runs the transformer for embeddings -> takes the resulting embedding -> generates the proper datastructure to upload to a vector database (pinecone) and does so in chuncks of 100 vectors, as per the pinecone documentation -> when finished with a file save to a log.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=str(PINECONE_API_KEY),\n",
    "    environment='gcp-starter'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case we need to delete and recreate the index to start fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone.delete_index('review-owl')\n",
    "# pinecone.create_index('review-owl', dimension=384, metric='euclidean', pods=1, pod_type='starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_POOL_THREADS = 30\n",
    "index = pinecone.Index('review-owl', pool_threads=PINECONE_POOL_THREADS)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "# alternative embedding model to consider\n",
    "# model = SentenceTransformer('BAAI/llm-embedder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing a specific repo dataset by finding all the files starting with the index number and a dash\n",
    "import glob\n",
    "\n",
    "# getting the index number from the file name\n",
    "def get_index_number(file):\n",
    "    \"\"\"A helper function to get the index number from the file name.\"\"\"\n",
    "    return int(file.split('\\\\')[1].split('-')[0])\n",
    "\n",
    "def import_filepaths(folder_path: str):\n",
    "    # getting all the files in the directory\n",
    "    file_paths = glob.glob(folder_path)\n",
    "\n",
    "    # sorting the files by the index number\n",
    "    sorted_filepaths = sorted(file_paths, key=get_index_number)\n",
    "\n",
    "    return sorted_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-base-en-v1.5\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5\"\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-small-en-v1.5\"\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/BAAI/llm-embedder\"\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}\n",
    "\n",
    "def file_to_embedding_inputs(file_path: str):\n",
    "    embedding_input = []\n",
    "    \n",
    "    data = pd.read_json(file_path, orient='index')\n",
    "    for row in tqdm(data.iloc[:, 0], 'Splitting file into payloads', position=2):\n",
    "        embedding_input.append('path: ' + row['path'] + '\\n' + 'diff_hunk: ' + row['diff_hunk'])\n",
    "    \n",
    "    return embedding_input\n",
    "\n",
    "def huggingface_inference_api_request(payload: list[str]):\n",
    "    try:\n",
    "        data = json.dumps(payload)\n",
    "        response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "\n",
    "        while (response.status_code != 200 and 'is currently loading' in response.content.decode(\"utf-8\")):\n",
    "            time.sleep(20)\n",
    "            print(response.content.decode(\"utf-8\"))\n",
    "            response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "\n",
    "        return json.loads(response.content.decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "def make_huggingface_request_with_backoff(payload: list[str]):\n",
    "    max_retries = 10\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = huggingface_inference_api_request(payload)\n",
    "            return response\n",
    "        except requests.HTTPError as e:\n",
    "            print('HTTP error: ' + str(e.response.status_code))\n",
    "            if e.response.status_code == 503:\n",
    "                # Handle rate-limiting error\n",
    "                wait_time = (2 ** retries) + (random.uniform(0, 1) * 0.1)  # Exponential backoff with random jitter\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "            else:\n",
    "                print('Error: ' + str(e))\n",
    "                # If it's not a 503 error, re-raise the exception\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions\n",
    "            print(e)\n",
    "            retries += 1\n",
    "\n",
    "    # If all retries fail\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(iterable: list, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunks = []\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        chunks.append(list(itertools.islice(it, batch_size)))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def pinecone_upsert_request(index, payload):\n",
    "    try:\n",
    "        data = json.dumps(payload)\n",
    "        response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "        if (response.status_code != 200): raise Exception(response.json())\n",
    "        return json.loads(response.content.decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n",
    "\n",
    "def make_pinecone_upsert_with_backoff(index, payload):\n",
    "    max_retries = 3\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            pinecone_responses = pinecone_upsert_request(index, payload)\n",
    "            return pinecone_responses\n",
    "        except requests.HTTPError as e:\n",
    "            if e.response.status_code == 503:\n",
    "                # Handle rate-limiting error\n",
    "                wait_time = (2 ** retries) + (random.uniform(0, 1) * 0.1)  # Exponential backoff with random jitter\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "            else:\n",
    "                # If it's not a 503 error, re-raise the exception\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions\n",
    "            retries += 1\n",
    "\n",
    "    # If all retries fail\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "import hashlib\n",
    "\n",
    "def hash_id(id):\n",
    "    return hashlib.sha256(id.encode('utf-8')).hexdigest()\n",
    "\n",
    "def embed_file(file_path: str):\n",
    "    vectors_list = []\n",
    "    embedding_inputs_list = file_to_embedding_inputs(file_path)\n",
    "\n",
    "    for payload in tqdm(embedding_inputs_list, 'Generating vector embeddings', position=1):\n",
    "        # vectors_list.append(make_huggingface_request_with_backoff(payload))\n",
    "        vectors_list.append(model.encode(payload).tolist())\n",
    "    \n",
    "    return vectors_list\n",
    "\n",
    "def generate_metadata(file_path: str):\n",
    "    metadata_list = []\n",
    "    data = pd.read_json(file_path, orient='index')\n",
    "\n",
    "    for row in tqdm(data.iloc[:, 0], 'Splitting file into payloads', position=2):\n",
    "        metadata_list.append({'repo': data.columns[0],'path': row['path'], 'diff': row['diff_hunk'], 'body': row['body']})\n",
    "    \n",
    "    return metadata_list\n",
    "\n",
    "def generate_vector_ids(metadata_list: list[dict]):\n",
    "    ids = []\n",
    "    for i in tqdm(range(len(metadata_list)), 'Generating vector ids', position=2):\n",
    "        ids.append(metadata_list[i]['repo'] + '-' + hash_id(str(i)))\n",
    "    return ids\n",
    "\n",
    "def generate_upsert_data(file_path: str):\n",
    "    embedding_result = embed_file(file_path)\n",
    "    metadata_list = generate_metadata(file_path)\n",
    "    vector_ids = generate_vector_ids(metadata_list)\n",
    "\n",
    "    upsert_data = list(zip(vector_ids, embedding_result, metadata_list))\n",
    "\n",
    "    return upsert_data\n",
    "\n",
    "def worker(file_queue: queue.Queue):\n",
    "    while True:\n",
    "        file_path = file_queue.get()\n",
    "        if file_path is None:\n",
    "            break\n",
    "\n",
    "        upsert_data = generate_upsert_data(file_path)\n",
    "\n",
    "        upsert_data_chunks = split_into_chunks(upsert_data)\n",
    "\n",
    "        for chunk in tqdm(upsert_data_chunks, 'Uploading data to Pinecone', position=1):\n",
    "            index.upsert(chunk)\n",
    "        print(f\"Logging: {file_path} processed\")\n",
    "\n",
    "        file_queue.task_done()\n",
    "\n",
    "# Function to create and manage worker threads\n",
    "def create_worker_threads(num_threads: int, file_queue: queue.Queue):\n",
    "    threads = list[threading.Thread]()\n",
    "\n",
    "    for _ in range(num_threads):\n",
    "        thread = threading.Thread(target=worker, args=(file_queue,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    return threads\n",
    "\n",
    "# Main function to process files using multiple threads\n",
    "def process_files_with_threads(file_names: list[str], num_threads: int, start: int = 0, end=None):\n",
    "    # Create a thread-safe queue\n",
    "    file_queue = queue.Queue()\n",
    "\n",
    "    # Populate the queue with file names\n",
    "    for file_name in file_names[start:end]:\n",
    "        file_queue.put(file_name)\n",
    "\n",
    "    # Create worker threads\n",
    "    threads = create_worker_threads(num_threads, file_queue)\n",
    "\n",
    "    # Wait for all file processing to be completed\n",
    "    file_queue.join()\n",
    "\n",
    "    # Stop the worker threads\n",
    "    for _ in range(num_threads):\n",
    "        file_queue.put(None)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "# file_paths = import_filepaths('dataset/mined-comments-25stars-25prs-JavaScript.json/repo-split/*.json')\n",
    "# upsert_data = generate_upsert_data(file_paths[3500])\n",
    "# upsert_data_chunks = split_into_chunks(upsert_data)\n",
    "# len(upsert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = import_filepaths('dataset/mined-comments-25stars-25prs-JavaScript.json/repo-split/*.json')\n",
    "process_files_with_threads(file_names=file_paths, num_threads=2, start=6000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
